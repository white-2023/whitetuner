"""
è°ƒè¯• Block Swap 22 vs 23 çš„åŒºåˆ«
åˆ†æä¸ºä»€ä¹ˆ 23 ä¸ª block swap ä¼šæŠ¥é”™
"""

import torch
import sys
sys.path.insert(0, "D:/ai/whitetuner/whitetuner_diffusers")


def analyze_block_swap_logic(num_blocks: int, blocks_to_swap: int):
    """åˆ†æ block swap çš„é€»è¾‘ï¼Œæ‰“å°æ¯ä¸ª block çš„ hook è¡Œä¸º"""
    print(f"\n{'=' * 70}")
    print(f"åˆ†æ Block Swap é€»è¾‘: num_blocks={num_blocks}, blocks_to_swap={blocks_to_swap}")
    print(f"{'=' * 70}")
    
    # åˆå§‹çŠ¶æ€
    blocks_on_gpu = num_blocks - blocks_to_swap
    print(f"\n[åˆå§‹çŠ¶æ€]")
    print(f"  GPU ä¸Šçš„ blocks: 0 ~ {blocks_on_gpu - 1} (å…± {blocks_on_gpu} ä¸ª)")
    print(f"  CPU ä¸Šçš„ blocks: {blocks_on_gpu} ~ {num_blocks - 1} (å…± {blocks_to_swap} ä¸ª)")
    
    # Forward é˜¶æ®µçš„äº¤æ¢
    print(f"\n[Forward é˜¶æ®µ] - æ¯ä¸ª block æ‰§è¡Œåè§¦å‘çš„äº¤æ¢")
    forward_swaps = []
    for block_idx in range(num_blocks):
        # submit_move_blocks_forward çš„é€»è¾‘ (supports_backward=True)
        if block_idx >= blocks_to_swap:
            continue
        block_idx_to_cpu = block_idx
        block_idx_to_cuda = num_blocks - blocks_to_swap + block_idx
        forward_swaps.append((block_idx, block_idx_to_cpu, block_idx_to_cuda))
        print(f"  Block {block_idx} æ‰§è¡Œå: block {block_idx_to_cpu} â†’ CPU, block {block_idx_to_cuda} â†’ GPU")
    
    # Forward ç»“æŸåçš„çŠ¶æ€
    cpu_blocks = set(range(blocks_to_swap))  # è¢«äº¤æ¢å‡ºå»çš„
    gpu_blocks = set(range(blocks_to_swap, num_blocks))  # è¢«äº¤æ¢å›æ¥çš„ + ä»æœªç§»åŠ¨çš„
    print(f"\n[Forward ç»“æŸåçŠ¶æ€]")
    print(f"  CPU ä¸Šçš„ blocks: {sorted(cpu_blocks)}")
    print(f"  GPU ä¸Šçš„ blocks: {sorted(gpu_blocks)}")
    
    # Backward é˜¶æ®µçš„ hook åˆ†æï¼ˆä½¿ç”¨ä¿®å¤åçš„é€»è¾‘ï¼‰
    print(f"\n[Backward é˜¶æ®µ] - æ¯ä¸ª block çš„ backward hook è¡Œä¸º")
    print(f"  (backward é¡ºåº: {num_blocks - 1} â†’ 0)")
    
    backward_hooks = []
    num_gpu_blocks = num_blocks - blocks_to_swap
    
    for block_index in range(num_blocks):
        num_blocks_propagated = num_blocks - block_index - 1
        
        # ä¿®å¤è¾¹ç•Œæƒ…å†µï¼šå½“ GPU ä¸Šåªæœ‰ 1 ä¸ª block æ—¶ï¼Œä½¿ç”¨ç®€åŒ–é€»è¾‘
        if num_gpu_blocks == 1:
            if block_index > 0:
                swapping = True
                block_idx_to_cpu = block_index
                block_idx_to_cuda = block_index - 1
            else:
                swapping = False
                block_idx_to_cpu = 0
                block_idx_to_cuda = 0
        else:
            swapping = num_blocks_propagated > 0 and num_blocks_propagated <= blocks_to_swap
            block_idx_to_cpu = num_blocks - num_blocks_propagated
            block_idx_to_cuda = blocks_to_swap - num_blocks_propagated
        
        waiting = block_index > 0 and block_index <= blocks_to_swap
        block_idx_to_wait = block_index - 1
        
        if not swapping and not waiting:
            continue
        
        backward_hooks.append({
            'block_index': block_index,
            'swapping': swapping,
            'waiting': waiting,
            'swap_to_cpu': block_idx_to_cpu if swapping else None,
            'swap_to_cuda': block_idx_to_cuda if swapping else None,
            'wait_for': block_idx_to_wait if waiting else None,
        })
    
    # æŒ‰ backward æ‰§è¡Œé¡ºåºæ‰“å°
    for hook in sorted(backward_hooks, key=lambda x: -x['block_index']):
        print(f"  Block {hook['block_index']:2d}: ", end="")
        if hook['swapping']:
            print(f"swap(block {hook['swap_to_cpu']} â†’ CPU, block {hook['swap_to_cuda']} â†’ GPU) ", end="")
        if hook['waiting']:
            print(f"wait(block {hook['wait_for']})", end="")
        print()
    
    # æ‰¾å‡ºé—®é¢˜ï¼šå“ªäº› block åœ¨ backward æ—¶å¯èƒ½æƒé‡ä¸åœ¨ GPU
    print(f"\n[é—®é¢˜åˆ†æ] - æ£€æŸ¥ backward æ—¶æƒé‡ä½ç½®")
    
    # æ¨¡æ‹Ÿ backward è¿‡ç¨‹
    current_cpu = set(cpu_blocks)
    current_gpu = set(gpu_blocks)
    pending_moves = {}  # block_idx_to_cuda -> (from_cpu_block)
    
    print(f"  Backward å¼€å§‹æ—¶: GPU={sorted(current_gpu)}, CPU={sorted(current_cpu)}")
    
    for block_index in range(num_blocks - 1, -1, -1):
        # æ£€æŸ¥å½“å‰ block æ˜¯å¦éœ€è¦ recomputeï¼ˆgradient checkpointingï¼‰
        if block_index not in current_gpu:
            print(f"\n  âš ï¸  Block {block_index} éœ€è¦ recompute ä½†æƒé‡åœ¨ CPU!")
            print(f"      è¿™ä¼šå¯¼è‡´ RuntimeError: Expected all tensors to be on the same device")
        
        # æ‰¾åˆ°è¿™ä¸ª block çš„ hook ä¿¡æ¯
        hook = None
        for h in backward_hooks:
            if h['block_index'] == block_index:
                hook = h
                break
        
        if hook is None:
            continue
        
        # æ‰§è¡Œ swap æ“ä½œï¼ˆæäº¤åˆ°åå°çº¿ç¨‹ï¼‰
        if hook['swapping']:
            pending_moves[hook['swap_to_cuda']] = hook['swap_to_cpu']
        
        # æ‰§è¡Œ wait æ“ä½œ
        if hook['waiting']:
            wait_block = hook['wait_for']
            if wait_block in pending_moves:
                # å®Œæˆç§»åŠ¨
                from_cpu_block = pending_moves.pop(wait_block)
                current_cpu.add(from_cpu_block)
                current_gpu.discard(from_cpu_block)
                current_gpu.add(wait_block)
                current_cpu.discard(wait_block)
    
    return forward_swaps, backward_hooks


def test_block_swap_with_model(blocks_to_swap: int, model_path: str = None):
    """å®é™…æµ‹è¯• block swap"""
    from flux2_modules import load_flux2_transformer_from_diffusers
    
    device = torch.device("cuda:0")
    dtype = torch.bfloat16
    
    if model_path is None:
        model_path = "F:/models/FLUX.2-klein-base-9B"
    
    print(f"\n{'=' * 70}")
    print(f"å®é™…æµ‹è¯• Block Swap: blocks_to_swap={blocks_to_swap}")
    print(f"{'=' * 70}")
    
    print("\nåŠ è½½æ¨¡å‹...")
    transformer = load_flux2_transformer_from_diffusers(model_path, torch_dtype=dtype, device="cpu")
    
    num_single_blocks = transformer.num_single_blocks
    print(f"æ¨¡å‹æœ‰ {num_single_blocks} ä¸ª single blocks")
    
    # å¯ç”¨ block swap
    print(f"\nå¯ç”¨ block swap: {blocks_to_swap} blocks...")
    try:
        transformer.enable_block_swap(
            blocks_to_swap=blocks_to_swap,
            device=device,
            supports_backward=True,
            use_pinned_memory=False,
        )
    except AssertionError as e:
        print(f"âŒ å¯ç”¨ block swap å¤±è´¥: {e}")
        return False
    
    # ç§»åŠ¨æ¨¡å‹
    transformer.move_to_device_except_swap_blocks(device)
    transformer.prepare_block_swap_before_forward()
    
    # å¯ç”¨ gradient checkpointing
    print("å¯ç”¨ gradient checkpointing (with activation cpu offloading)...")
    transformer.enable_gradient_checkpointing(activation_cpu_offloading=True)
    
    # å‡†å¤‡è¾“å…¥
    num_attention_heads = transformer.config.get("num_attention_heads", 48)
    attention_head_dim = transformer.config.get("attention_head_dim", 128)
    joint_attention_dim = transformer.config.get("joint_attention_dim", 15360)
    in_channels = transformer.config.get("in_channels", 128)
    
    batch_size = 1
    img_seq_len = 64  # å‡å°ä»¥åŠ é€Ÿæµ‹è¯•
    txt_seq_len = 32
    
    hidden_states = torch.randn(batch_size, img_seq_len, in_channels, device=device, dtype=dtype, requires_grad=True)
    encoder_hidden_states = torch.randn(batch_size, txt_seq_len, joint_attention_dim, device=device, dtype=dtype)
    timestep = torch.tensor([0.5], device=device, dtype=dtype)
    
    img_ids = torch.zeros(img_seq_len, 4, device=device, dtype=dtype)
    for i in range(img_seq_len):
        h = i // 8
        w = i % 8
        img_ids[i] = torch.tensor([0, 0, h, w], dtype=dtype)
    
    txt_ids = torch.zeros(txt_seq_len, 4, device=device, dtype=dtype)
    
    # æµ‹è¯• forward + backward
    print("\næ‰§è¡Œ forward + backward...")
    try:
        # Forward
        with torch.autocast(device_type="cuda", dtype=torch.bfloat16):
            output = transformer(
                hidden_states=hidden_states,
                encoder_hidden_states=encoder_hidden_states,
                timestep=timestep,
                img_ids=img_ids,
                txt_ids=txt_ids,
                guidance=None,
            )
        print(f"  Forward æˆåŠŸ: output shape={output['sample'].shape}")
        
        # Backward - ä¸èƒ½åœ¨è¿™é‡Œè°ƒç”¨ prepare_block_swap_before_forwardï¼
        # å› ä¸º backward éœ€è¦åœ¨ forward äº¤æ¢åçš„çŠ¶æ€ä¸‹æ‰§è¡Œ
        loss = output['sample'].mean()
        loss.backward()
        print(f"  Backward æˆåŠŸ: grad norm={hidden_states.grad.norm():.4f}")
    except Exception as e:
        print(f"âŒ Forward/Backward å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    # æ¸…ç†
    transformer.cleanup_offloader()
    del transformer
    torch.cuda.empty_cache()
    
    return True


def simulate_backward_execution(num_blocks: int, blocks_to_swap: int):
    """æ¨¡æ‹Ÿ backward æ‰§è¡Œè¿‡ç¨‹ï¼Œè¿½è¸ªæ¯ä¸ª block çš„æƒé‡ä½ç½®"""
    print(f"\n{'=' * 70}")
    print(f"æ¨¡æ‹Ÿ Backward æ‰§è¡Œ: num_blocks={num_blocks}, blocks_to_swap={blocks_to_swap}")
    print(f"{'=' * 70}")
    
    # Forward ç»“æŸåçš„åˆå§‹çŠ¶æ€
    cpu_blocks = set(range(blocks_to_swap))
    gpu_blocks = set(range(blocks_to_swap, num_blocks))
    
    # æ”¶é›†æ‰€æœ‰ hook ä¿¡æ¯ï¼ˆä½¿ç”¨ä¿®å¤åçš„é€»è¾‘ï¼‰
    hooks = {}
    num_gpu_blocks = num_blocks - blocks_to_swap
    
    for block_index in range(num_blocks):
        num_blocks_propagated = num_blocks - block_index - 1
        
        # ä¿®å¤è¾¹ç•Œæƒ…å†µï¼šå½“ GPU ä¸Šåªæœ‰ 1 ä¸ª block æ—¶ï¼Œä½¿ç”¨ç®€åŒ–é€»è¾‘
        if num_gpu_blocks == 1:
            if block_index > 0:
                swapping = True
                swap_to_cpu = block_index
                swap_to_cuda = block_index - 1
            else:
                swapping = False
                swap_to_cpu = None
                swap_to_cuda = None
        else:
            swapping = num_blocks_propagated > 0 and num_blocks_propagated <= blocks_to_swap
            swap_to_cpu = num_blocks - num_blocks_propagated if swapping else None
            swap_to_cuda = blocks_to_swap - num_blocks_propagated if swapping else None
        
        waiting = block_index > 0 and block_index <= blocks_to_swap
        
        if swapping or waiting:
            hooks[block_index] = {
                'swapping': swapping,
                'waiting': waiting,
                'swap_to_cpu': swap_to_cpu,
                'swap_to_cuda': swap_to_cuda,
                'wait_for': block_index - 1 if waiting else None,
            }
    
    # æ¨¡æ‹Ÿ futures å­—å…¸
    futures = {}  # block_idx_to_cuda -> True (è¡¨ç¤ºå·²æäº¤)
    
    print("\n[Backward æ‰§è¡Œé¡ºåº] (ä» block 47 â†’ 0)")
    print("-" * 70)
    
    errors = []
    
    # Backward ä» 47 â†’ 0
    for block_index in range(num_blocks - 1, -1, -1):
        # Step 1: Recompute (éœ€è¦æƒé‡åœ¨ GPU)
        if block_index not in gpu_blocks:
            error_msg = f"Block {block_index} recompute å¤±è´¥: æƒé‡åœ¨ CPU!"
            errors.append((block_index, "recompute", error_msg))
            print(f"  âŒ Block {block_index}: RECOMPUTE å¤±è´¥ - æƒé‡åœ¨ CPU!")
            print(f"     å½“å‰ GPU blocks: {sorted(gpu_blocks)}")
            print(f"     å½“å‰ CPU blocks: {sorted(cpu_blocks)}")
            continue
        
        # Step 2: Backward (å‡è®¾æˆåŠŸ)
        
        # Step 3: Backward Hook
        if block_index in hooks:
            hook = hooks[block_index]
            actions = []
            
            # å¤„ç† swapping
            if hook['swapping']:
                to_cpu = hook['swap_to_cpu']
                to_cuda = hook['swap_to_cuda']
                futures[to_cuda] = (to_cpu, to_cuda)  # è®°å½•å¾…å¤„ç†çš„ç§»åŠ¨
                actions.append(f"æäº¤ block {to_cpu}â†’CPU, block {to_cuda}â†’GPU")
            
            # å¤„ç† waiting
            if hook['waiting']:
                wait_block = hook['wait_for']
                if wait_block in futures:
                    # å®Œæˆç§»åŠ¨
                    from_cpu, to_cuda = futures.pop(wait_block)
                    cpu_blocks.add(from_cpu)
                    gpu_blocks.discard(from_cpu)
                    gpu_blocks.add(to_cuda)
                    cpu_blocks.discard(to_cuda)
                    actions.append(f"ç­‰å¾… block {wait_block} å®Œæˆ (block {to_cuda} â†’ GPU)")
                else:
                    actions.append(f"ç­‰å¾… block {wait_block} ä½† futures ä¸­ä¸å­˜åœ¨!")
                    errors.append((block_index, "wait", f"futures[{wait_block}] ä¸å­˜åœ¨"))
            
            if actions:
                print(f"  Block {block_index:2d}: recompute âœ“ â†’ backward âœ“ â†’ hook: {', '.join(actions)}")
        else:
            print(f"  Block {block_index:2d}: recompute âœ“ â†’ backward âœ“ â†’ (æ—  hook)")
    
    print()
    if errors:
        print("=" * 70)
        print("âš ï¸  å‘ç°é—®é¢˜:")
        print("=" * 70)
        for block_idx, stage, msg in errors:
            print(f"  Block {block_idx} ({stage}): {msg}")
    else:
        print("âœ… æ¨¡æ‹Ÿæ‰§è¡Œå®Œæˆï¼Œæ— é”™è¯¯")
    
    return errors


def analyze_recompute_issue():
    """åˆ†æ gradient checkpointing recompute æ—¶çš„é—®é¢˜"""
    print("\n" + "=" * 70)
    print("é—®é¢˜åˆ†æ: Gradient Checkpointing + Block Swap")
    print("=" * 70)
    
    print("""
é—®é¢˜æ ¹æº:
---------
1. Gradient checkpointing åœ¨ backward æ—¶éœ€è¦ recompute forward
2. Backward hook æ˜¯åœ¨ block çš„ backward å®Œæˆåæ‰è§¦å‘çš„
3. Block N çš„ recompute å‘ç”Ÿåœ¨ Block (N+1) çš„ hook æ‰§è¡Œä¹‹å

å…³é”®æ—¶åºé—®é¢˜ (blocks_to_swap=23):
---------------------------------
1. Forward ç»“æŸå: Block 22 åœ¨ CPU, Block 23 åœ¨ GPU
2. Backward åˆ° Block 46 æ—¶: Hook æäº¤ "Block 22 â†’ GPU" åˆ°åå°çº¿ç¨‹
3. Backward åˆ° Block 23 æ—¶: Hook æ‰§è¡Œ wait(Block 22)ï¼Œç¡®ä¿ Block 22 åœ¨ GPU
4. ç„¶å Block 22 çš„ recompute å¼€å§‹æ‰§è¡Œ

é—®é¢˜åœ¨äº:
---------
- è™½ç„¶ Block 23 çš„ hook ä¼š wait(Block 22)
- ä½† Block 22 çš„ç§»åŠ¨æ˜¯åœ¨ Block 46 çš„ hook ä¸­å¼‚æ­¥æäº¤çš„
- å¦‚æœ CUDA stream åŒæ­¥æœ‰é—®é¢˜ï¼ŒBlock 22 çš„æƒé‡å¯èƒ½è¿˜æ²¡å®Œå…¨åˆ° GPU

å¯èƒ½çš„ä¿®å¤æ–¹æ¡ˆ:
---------------
1. åœ¨ block.forward() å¼€å§‹æ—¶æ£€æŸ¥å¹¶ç­‰å¾…æƒé‡åˆ°ä½
2. ä¿®æ”¹ backward hook ç¡®ä¿æå‰æäº¤ç§»åŠ¨æ“ä½œ
3. å¢åŠ é¢å¤–çš„åŒæ­¥ç‚¹
""")


def main():
    import argparse
    parser = argparse.ArgumentParser(description="è°ƒè¯• Block Swap é—®é¢˜")
    parser.add_argument("--mode", type=str, default="analyze", choices=["analyze", "simulate", "test", "all"])
    parser.add_argument("--model_path", type=str, default=None)
    parser.add_argument("--blocks", type=int, nargs='+', default=[22, 23])
    parser.add_argument("--num_blocks", type=int, default=24, help="single blocks æ•°é‡ (FLUX.2 Klein 9B=48, å°æ¨¡å‹=24)")
    args = parser.parse_args()
    
    num_single_blocks = args.num_blocks  # ä»å‚æ•°è·å–
    
    if args.mode in ["analyze", "all"]:
        # åˆ†æé€»è¾‘å·®å¼‚
        for blocks_to_swap in args.blocks:
            analyze_block_swap_logic(num_single_blocks, blocks_to_swap)
        
        analyze_recompute_issue()
    
    if args.mode in ["simulate", "all"]:
        # æ¨¡æ‹Ÿ backward æ‰§è¡Œ
        print("\n" + "#" * 70)
        print("# æ¨¡æ‹Ÿ Backward æ‰§è¡Œè¿‡ç¨‹")
        print("#" * 70)
        for blocks_to_swap in args.blocks:
            errors = simulate_backward_execution(num_single_blocks, blocks_to_swap)
            if errors:
                print(f"\nğŸ’¡ blocks_to_swap={blocks_to_swap} ä¼šå‡ºç°é—®é¢˜!")
            else:
                print(f"\nâœ… blocks_to_swap={blocks_to_swap} ç†è®ºä¸Šåº”è¯¥æ­£å¸¸")
    
    if args.mode in ["test", "all"]:
        # å®é™…æµ‹è¯•
        for blocks_to_swap in args.blocks:
            print(f"\n\n{'#' * 70}")
            print(f"# æµ‹è¯• blocks_to_swap = {blocks_to_swap}")
            print(f"{'#' * 70}")
            
            success = test_block_swap_with_model(blocks_to_swap, args.model_path)
            if success:
                print(f"\nâœ… blocks_to_swap={blocks_to_swap} æµ‹è¯•é€šè¿‡")
            else:
                print(f"\nâŒ blocks_to_swap={blocks_to_swap} æµ‹è¯•å¤±è´¥")


if __name__ == "__main__":
    main()

